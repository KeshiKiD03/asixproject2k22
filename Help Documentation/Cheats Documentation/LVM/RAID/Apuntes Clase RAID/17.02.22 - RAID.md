-----------
-- PLANTILLA --
-----------

<!---
# Plantilla H1
## Plantilla H2
### Plantilla H3
-->
<!--- <img src="https://phoneky.co.uk/thumbs/screensavers/down/original/linux_3rj131p8.gif" />
-->

‚≠êÔ∏è **PLANTILLA** ‚≠êÔ∏è

| üî•PLANTILLA TALBA‚ùóüî• | 
| ------------- |
| *Plantilla* |


--------------------------------------------------------------------------------

## 8 semanas

# UF1 - DATA AT REST

## ACL

## LVM [APUNTES]

## RAID

### EXEMPLE 1 / EJERCICIO 1 DE RAID (HA CAMBIADO ALGO)

[apuntes_sucio]

* losetup /dev/loop1

* mdadm --create /dev/

root@i11:/opt/lvm# losetup /dev/loop2 disk01.img 
root@i11:/opt/lvm# losetup /dev/loop3 disk02.img 
root@i11:/opt/lvm# losetup /dev/loop4 disk03.img 
root@i11:/opt/lvm# losetup /dev/loop5 disk04.img 
root@i11:/opt/lvm# losetup /dev/loop6 disk05.img 
root@i11:/opt/lvm# losetup /dev/loop7 disk500.img 
root@i11:/opt/lvm# losetup -a
/dev/loop1: [66306]:4333635 (/var/lib/snapd/snaps/msnake_10.snap)
/dev/loop6: [66306]:5245314 (/opt/lvm/disk05.img)
/dev/loop4: [66306]:5245294 (/opt/lvm/disk03.img)
/dev/loop2: [66306]:5245283 (/opt/lvm/disk01.img)
/dev/loop0: [66306]:4333633 (/var/lib/snapd/snaps/core_11993.snap)
/dev/loop7: [66306]:5245315 (/opt/lvm/disk500.img)
/dev/loop5: [66306]:5245313 (/opt/lvm/disk04.img)
/dev/loop3: [66306]:5245290 (/opt/lvm/disk02.img)
root@i11:/opt/lvm# 

* mdadm --create /dev/md/backup --level=1 --raid-devices=2 /dev/loop4 /dev/loop2 --spare-devices=1 /dev/loop3


* cat /proc/mdstat

* tree /dev/disk 

* mdadm --detail /dev/md0




* mdadm --examine --scan -->>>>> UUID=.....

* mdadm --examine --scan > /etc/mdadm/mdadm.conf


* mdadm --stop /dev/md/backup

* mdadm --assemble --scan --> Enciende lo que ha encontrado.

* cat /proc/mdstat

* mdadm --stop /dev/md/backup 

* mdadm --assemble /dev/md/backup

* cat /proc/mdstat

* /dev/md/backup --> Tenemos el fichero de configuraci√≥n que le indicamos donde tenemos que encender.

* mv /etc/mdadm/mdadm.conf /etc/mdadm/mdadm.conf.no

* mdadm --assemble /dev/md/backup

* Por encenderlo por NOMBRE no VA. Va a buscaral fichero de configuraci√≥n.

* Est√° parado.

Hay otro mecanimos para encender el RAID.

* mdadm --create /dev/md/backup /dev/loop1 /devloop2 ---> lo enciende

* cat /proc/mdstat --> Enciende el RAID con 2 elementos.

mdactive [loop1] [loop2]

* mdadm --stop /dev/md/backup

* mdadm --assemble /dev/md/backup /dev/loop1 /dev/loop2 /dev/loop3

* Quedan 2 grupos.


--

Creamos otro RAID

* mdadm --create /dev/md/live --level=1 --raid-devices=2 /dev/loop4 /dev/loop5

    * No quedan discos de SPARE

    * level=1 = RAID1

* y

* cat /proc/mdstat

    * Hay 2 RAIDS (md126 md127 (?))

* mdadm --examine --scan --> Se lo pasamos al FICHERO de configuraci√≥n.

* mdadm --examine --scan > /etc/mdadm/mdadm.conf

* mdadm --stop /dev/md/backup /dev/md/live

mdadm: stoppped /dev/md/backup y live

* mdadm --examine --scan --> Aunque diga SCAN, y EXAMINE, si existe el fichero de conf, PRIMERO MIRA EL FICHERO DE CONFIGURACI√ìN, LUEGO EL SCAN.

* MDADM --assemble --scan --> Enciende los 2 RAIDS

* mdadm --stop /dev/md/backup /dev/md/live

.--
* mdadm --stop --scan

* /cat/proc/mdstat
--

* mdadm --assemble /dev/md/backup --> Enciende solo el MD BACKUP

* cp /etc/mdadm/mdadm.conf /etc/mdadm/mdadm.conf.no


* vim /etc/mdadm/mdadm.conf

HA EDITADO EL FICHERO (?) Ha eliminado uno de ellos

* mdadm --assemble --scan --> Solo enciende lo que hay dentro del fichero de configuraci√≥n!!!!

Es verdad?

* Antes hay que parar el RAID

* mdadm --stop --scan

* rm /etc/mdadm/mdadm.conf

* mdadm --assemble --scan --> Ha buscado todos los SEGELLS de los discos que ha encontrado y ha intentado montar las coasas.

mdadm: /dev/md/live

mdadm: /dev/md/backup

* 

* 

* 

* 

* 

* 

* 

* 

* 

* 

* 

* 

* 

* 

### PAG 29 PR√ÅCTICA 3

* mdadm --stop --scan

RAIDS se llamaban 127 y 126

* mdadm --create /dev/md/myraid --level=1 --raid-devices=2 /dev/loop1 /dev/loop2

Va cambiando de nombre por los FINGERPRINTS.

* cat /proc/mdstat

* mdadm --grow /dev/md/myraid --raid-devices=3 --add /dev/loop3 --spare-devices=1 -add /dev/loop4

Con GROW no podemos a√±adir dispositivos de SPARE

* mdadm --grow /dev/md/myraid --raid-devices=3 --add /dev/loop3

raid_disks for /dev/md/myraid set to 3

* cat /proc/mdstat

* mdadm --detail /dev/md/myraid

Raid Devices

Total Devices

Active Devices

Working Devices

* mdadm /dev/md/myraid -add /dev/loop4

cat /proc/mdstat

md127 loop4[3](S) --> Lo a√±ade al SPARE.

* Con el GROW ampliamos los discos RAID de un grupo

* --add a√±adimos discos.

* Hay que saber cuantos hay que mantener sincronizados.

* Pasar de un RAID 1 a 2 discos de RAID.


---

* cat /proc/mdstat

* Simular un error en el LOOP3 y eliminar el disco



* 1. Simular el FAIL y 2. Borrar el disco


* mdadm /dev/md/myraid --remove /dev/loop3

* mdadm --grow /dev/md/myraid --level=1 --raid-devices=2 /dev/loop1 /dev/loop2

* cat /proc/mdstat --> Hay 3 dispositivos activos. 

loop4 loop2 loop1

* Hacer un error y extrar el loop4

* mdadm /dev/md/myraid --fail /dev/loop4

* mdadm /dev/md/myraid --remove /dev/loop4

* cat /proc/mdstat

* 

* mdadm --grow /dev/md/myraid --level=1 --raid-devices=2

Set to to2


* Para pasar 5 a 2 --> 5 Discos a 2 DISCOS. 2 a 5 --> Treure (REMOVE) y GROW. 

* mdadm --stop /dev/md/myraid

* mdadm --assemble --scan

* cat /proc/mdstat


------------------

# Crear otro disco para RAID  5



1. dd if=/dev/zero of=disk06.img bs=1k count=100k

2. losetup /dev/loop6 disk06.img

Pasar de un RAID 5 que tiene 3 discos + 1 SPARE a 5 DISCOS + 1 spare.

M√≠nimo 3 para un RAID 5. 3 discos tendr√°n 200M y 5 discos tendr√°n 400M.

3. mdadm --craete /dev/md/myraid --level=5 --raid-devices=3 /dev/loop1 /dev/loop2 /dev/loop3 --spare-devices=1 /dev/loop4

4. Y --> Yes

5. mdadm --grow /dev/md/myraid --raid-devices=5 --add /dev/loop5 /dev/loop6

6. cat /proc/mdstat

7. mdadm --create /dev/md/myraid --level=1 --raid-devices=2 /dev/loop1 /dev/loop2 --spare-devices=1 /dev/loop3

8. Tenemos un RAID 1 de 2 discos. 

9. Queremos convertirlo a UN RAID de nivel 5.

10. SPARE = RECAMBIO.

11. mdadm --grow /dev/md/myraid --level=5 --> Lo convierte a nivel 5

12. cat /proc/mdstat

active raid5 loop3[2](S)... --> Es un RAID 5 pero en verdad est√° con 2 DISCOS. loop2[1] loop1[0]

13. mdadm --detail /dev/md/myraid

Formado por 2 discos activos y 3.

State clean.

14. mdadm --grow /dev/md/myraid --raid-devices=3

15. mdadm --stop --scan

16.  mdadm --craete /dev/md/myraid --level=1 --raid-devices=3 /dev/loop1 /dev/loop2 /dev/loop3 (Sin SPARE)

17. mdadm --grow /dev/md/myraid --level=5 --> S√≥lo se pueden convertir 2 DISCOS ARRAY a RAID 5. **Solo lo deja con 2 discos.** *En principio deber√≠a funcionar*

18. mdadm --stop --scan



---------------------------------

1. dd if=/dev/zero of=disk600.img bs=1k count=500k

2. losetup /dev/loop7 disk600.img

3. **tree /sys/block --> Vemos los resultados de loop**

4. Raid de 2 DISCOS de 100 + el SPARE de 500

5. losetup ---> Enlazar el de 500

6. losetup /dev/loop11 disk500.img

7. losetup /dev/loop12 disk600.img 

8. mdadm --create /dev/md/myraid --level=1 --raid-devices=2 /dev/loop1 /dev/loop2 --spare-devices=1 /dev/loop11

Dice que ya lo estamos haciendo servir. 

El disco de SPARE es un 1% m√°s grande que otros discos.

9. Y de YES.

10. Lo formateamos, lo montamos y le ponemos datos.

11. mkfs -t ext4 /dev/md/myraid

12. mount /dev/md/myraid /mnt

13. df -h --> 41M

14. cp /bin/x* /mnt

15. cp /bin/b* /mnt

16. df -h

17. Provocar un error en todos los discos del RAID.

18. mdadm /dev/md/myraid --fail /dev/loop1

19. mdadm /dev/md/myraid --remove /dev/loop1

20. cat /proc/mdstat

21. mdadm --detail /dev/md/myraid

22. mdadm /dev/md/myraid --add /dev/loop12

23. cat /proc/mdstat --> Est√° de SPARE

24. mdadm /etc/md/myraid --fail /dev/loop2

25. mdadm /dev/md/myraid --remove /dev/loop2

26. mdadm --detail /dev/md/myraid

27. Tenemos 2 discos de 500, pero eran discos de 100, se usan 100M solo.

28. Cuando definimos el RAID usabamos discos de 100.

29. Hacer crecer el RAID (P√°g 42)

30. mdadm --grow /dev/md/myraid --size=max --> El RAID de la mida m√°xima posible que lo permiten los DISCOS.

31. mdadm --detail /dev/md/myraid --> Ha credido al TOPE que son 500. Array SIze es de 500.

32. df -h --> Continua siendo de 100. 

33. Hay que incrementar el sistema de ficheros.

34. Resize2fs /dev/md/myraid --> Lo a√±ade al m√°ximo posible.

35. df -h --> /dev/md127 --> 486M

36. 

37. 

38. 

39. 

40. 

41. 

42. 

43. 

44. 

45. 

46. 

47. 


#### DEURES PER FER NOSALTRES

* Pag 45 - 46

* Eliminar la marca del RAID (Zero super Block)

* dmesg | grep RAID si durante el ARRANQUE han ido saliendo elementos del RAID.




* Pr√°ctica (1) RAID + LVM en LOOPS

    * RAID 1 DOS DISCS de 500.

    * Un segon RAID1 (Dos discs de 500M).

    * Primer RAID, crear PV y PG mydisc.

    * LV de 200M sistema, LV de 100M addes.

    * MKFS i MUNTAR i POSAR-HI DADES.

    * Incorporar al VG el segon RAID.

    * Incrementar SISTEMA +100M.

    * Provocar un FAIL a un DELS DISCOS del RAID1.

    * Eliminar completament el RAID1 del VG.





* Pr√°ctica 2 (REAL)

    CREAR SDA2: 10G, sda3: 4G, SDA4: 4G

    Crear un RAID1 amb SDA3 i SDA4

    Generar un VG anomenat DISKEDT amb SDA3 i SDA4.

    Generar un LV Sistema de 3G, Dades de 1G.

    Muntar i posar-hi DADES.

    Automatitzar el muntatge /mnt/dades, /mnt/sistema).

    Reboot i verificar que dades i sistema estan disponibles.